<!doctype html><html lang=en dir=ltr>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Introduction #  Nowadays, there&rsquo;re couples of way to deploy a k8s cluster in various of form factors, from experimental cluster with a single node to production deployment with hundreds of servers.
This memo is a note for deploy a &lsquo;product&rsquo; k8s cluster using kubespray - limited by the resources for the experiment, no dedicated storage nodes provisioned, and the networking is not covered as well.
   Node Role Etcd External IP Internal IP      node-101 Controller yes  192.">
<meta name=theme-color content="#FFFFFF">
<meta name=color-scheme content="light dark"><meta property="og:title" content="Deploy k8s Cluster Using kubespray">
<meta property="og:description" content="Introduction #  Nowadays, there&rsquo;re couples of way to deploy a k8s cluster in various of form factors, from experimental cluster with a single node to production deployment with hundreds of servers.
This memo is a note for deploy a &lsquo;product&rsquo; k8s cluster using kubespray - limited by the resources for the experiment, no dedicated storage nodes provisioned, and the networking is not covered as well.
   Node Role Etcd External IP Internal IP      node-101 Controller yes  192.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://braydenlee.github.io/posts/deploy_k8s_cluster_using_kubespray/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-12-25T00:00:00+00:00">
<meta property="article:modified_time" content="2021-12-25T00:00:00+00:00">
<title>Deploy k8s Cluster Using kubespray | Brayden's Blog</title>
<link rel=manifest href=/manifest.json>
<link rel=icon href=/favicon.png type=image/x-icon>
<link rel=stylesheet href=/book.min.46181bc93375ba932026e753b37c40e6ff8bb16a9ef770c78bcc663e4577b1ba.css integrity="sha256-RhgbyTN1upMgJudTs3xA5v+LsWqe93DHi8xmPkV3sbo=" crossorigin=anonymous>
<script defer src=/flexsearch.min.js></script>
<script defer src=/en.search.min.f93aae903af77f2002958cf6ebd7117de564ab7d00a31032f121f28a9ee3c4be.js integrity="sha256-+TqukDr3fyAClYz269cRfeVkq30AoxAy8SHyip7jxL4=" crossorigin=anonymous></script>
</head>
<body dir=ltr>
<input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control>
<main class="container flex">
<aside class=book-menu>
<div class=book-menu-content>
<nav>
<h2 class=book-brand>
<a class="flex align-center" href=/><span>Brayden's Blog</span>
</a>
</h2>
<div class=book-search>
<input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/>
<div class="book-search-spinner hidden"></div>
<ul id=book-search-results></ul>
</div>
<ul>
<li class=book-section-flat>
<a href=https://braydenlee.github.io/docs/example/>Brayden's Notes</a>
<ul>
<li>
<a href=https://braydenlee.github.io/docs/example/table-of-contents/>Table of Contents</a>
<ul>
<li>
<a href=https://braydenlee.github.io/docs/example/table-of-contents/with-toc/>With ToC</a>
</li>
<li>
<a href=https://braydenlee.github.io/docs/example/table-of-contents/without-toc/>Without ToC</a>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-4e46b01272d410b3a99461d79326ddf4 class=toggle>
<label for=section-4e46b01272d410b3a99461d79326ddf4 class="flex justify-between">
<a role=button>Collapsed</a>
</label>
<ul>
<li>
<a href=https://braydenlee.github.io/docs/example/collapsed/3rd-level/>3rd Level</a>
<ul>
<li>
<a href=https://braydenlee.github.io/docs/example/collapsed/3rd-level/4th-level/>4th Level</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class=book-section-flat>
<span>Shortcodes</span>
<ul>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/buttons/>Buttons</a>
</li>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/columns/>Columns</a>
</li>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/details/>Details</a>
</li>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/expand/>Expand</a>
</li>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/hints/>Hints</a>
</li>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/katex/>Katex</a>
</li>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/mermaid/>Mermaid</a>
</li>
<li>
<input type=checkbox id=section-d3fc1bf6d66cd84b896a0af9f40cb1d5 class=toggle>
<label for=section-d3fc1bf6d66cd84b896a0af9f40cb1d5 class="flex justify-between">
<a href=https://braydenlee.github.io/docs/shortcodes/section/>Section</a>
</label>
<ul>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/section/first-page/>First Page</a>
</li>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/section/second-page/>Second Page</a>
</li>
</ul>
</li>
<li>
<a href=https://braydenlee.github.io/docs/shortcodes/tabs/>Tabs</a>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<a href=/posts/>
Blog
</a>
</li>
</ul>
</nav>
<script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>
</div>
</aside>
<div class=book-page>
<header class=book-header>
<div class="flex align-center justify-between">
<label for=menu-control>
<img src=/svg/menu.svg class=book-icon alt=Menu>
</label>
<strong>Deploy k8s Cluster Using kubespray</strong>
<label for=toc-control>
<img src=/svg/toc.svg class=book-icon alt="Table of Contents">
</label>
</div>
<aside class="hidden clearfix">
<nav id=TableOfContents>
<ul>
<li><a href=#introduction>Introduction</a></li>
<li><a href=#prerequisites>Prerequisites</a></li>
<li><a href=#detailed-steps>Detailed Steps</a>
<ul>
<li><a href=#disable-swap>Disable swap</a></li>
<li><a href=#enable-passwordless-login-via-ssh>Enable passwordless login via ssh</a></li>
<li><a href=#enable-passwordless-sudo>Enable passwordless sudo</a></li>
<li><a href=#dependencies>Dependencies</a></li>
<li><a href=#download-kubespray>Download kubespray</a></li>
<li><a href=#configure-the-cluster-setup>Configure the cluster setup</a></li>
<li><a href=#required-workaround>Required Workaround</a></li>
</ul>
</li>
<li><a href=#provision-the-cluster>Provision the Cluster</a>
<ul>
<li><a href=#kubectl-conf-settings>Kubectl conf settings</a></li>
</ul>
</li>
<li><a href=#troubleshootings>Troubleshootings</a>
<ul>
<li><a href=#failed-to-download-container-images>Failed to download container images</a></li>
<li><a href=#node-102--node-103-failed-to-join-controller-node-101>Node-102 & Node-103 failed to join controller node-101</a></li>
<li><a href=#coredns-service-crashedbackoff>coredns service crashedbackoff</a></li>
</ul>
</li>
<li><a href=#reference>Reference</a></li>
</ul>
</nav>
</aside>
</header>
<article class=markdown>
<h1>
<a href=/posts/deploy_k8s_cluster_using_kubespray/>Deploy k8s Cluster Using kubespray</a>
</h1>
<h5>December 25, 2021</h5>
<div>
<a href=/categories/memo/>memo</a>,
<a href=/categories/tips/>tips</a>
</div>
<div>
<a href=/tags/kubespray/>kubespray</a>,
<a href=/tags/k8s/>k8s</a>,
<a href=/tags/installation/>installation</a>
</div>
<h2 id=introduction>
Introduction
<a class=anchor href=#introduction>#</a>
</h2>
<p>Nowadays, there&rsquo;re couples of way to deploy a k8s cluster in various of form factors, from experimental cluster with a single node to production deployment with hundreds of servers.</p>
<p>This memo is a note for deploy a &lsquo;product&rsquo; k8s cluster using kubespray - limited by the resources for the experiment, no dedicated storage nodes provisioned, and the networking is not covered as well.</p>
<table>
<thead>
<tr>
<th>Node</th>
<th>Role</th>
<th>Etcd</th>
<th>External IP</th>
<th>Internal IP</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>node-101</td>
<td>Controller</td>
<td>yes</td>
<td></td>
<td>192.168.8.101</td>
<td>ubuntu22.04</td>
</tr>
<tr>
<td>node-102</td>
<td>Controller</td>
<td>yes</td>
<td></td>
<td>192.168.8.102</td>
<td>ubuntu22.04</td>
</tr>
<tr>
<td>node-103</td>
<td>Controller & worker</td>
<td>yes</td>
<td></td>
<td>192.168.8.103</td>
<td>ubuntu22.04</td>
</tr>
</tbody>
</table>
<h2 id=prerequisites>
Prerequisites
<a class=anchor href=#prerequisites>#</a>
</h2>
<ol>
<li>3 servers, with reasonable CPU, memory and disk space</li>
<li>At least 2 Ethernet interfaces per server
<ol>
<li>One interface for Internet access as some packages are downloaded on the fly;</li>
<li>The other interface used for the cluster management, (as well as pods network in this setup);</li>
</ol>
</li>
<li>Assume ubuntu 22.04 installed;</li>
</ol>
<h2 id=detailed-steps>
Detailed Steps
<a class=anchor href=#detailed-steps>#</a>
</h2>
<h3 id=disable-swap>
Disable swap
<a class=anchor href=#disable-swap>#</a>
</h3>
<p>This is required on every node</p>
<pre><code>$ swapoff \-a
</code></pre>
<h3 id=enable-passwordless-login-via-ssh>
Enable passwordless login via ssh
<a class=anchor href=#enable-passwordless-login-via-ssh>#</a>
</h3>
<p>This is to enable the passwordless login to the target nodes, so this is required to run on the deployment server (where we run the kubespray).</p>
<p>Generate ssh keys by running ssh-keygen, simply press &ldquo;enter&rdquo; on asking the passphrase.</p>
<pre><code>$ ssh-keygen
</code></pre>
<p>Copy the ssh key id to target server, enter the password following the prompt.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ ssh<span style=color:#ae81ff>\-</span>copy<span style=color:#ae81ff>\-</span>id brayden@192.168.8.101
$ ssh<span style=color:#ae81ff>\-</span>copy<span style=color:#ae81ff>\-</span>id brayden@192.168.8.102
$ ssh<span style=color:#ae81ff>\-</span>copy<span style=color:#ae81ff>\-</span>id brayden@192.168.8.103  
</code></pre></div><h3 id=enable-passwordless-sudo>
Enable passwordless sudo
<a class=anchor href=#enable-passwordless-sudo>#</a>
</h3>
<p>Add the line in <strong>BOLD</strong>Â  as shown below in the %sudo section, for the user used to provision the k8s cluster. This is required on every node.</p>
<pre tabindex=0><code>$ sudo vim /etc/sudoers

\# Allow members of group sudo to execute any command
%sudoÂ Â  ALL=\(ALL:ALL\) ALL
brayden ALL=\(ALL:ALL\) NOPASSWD:ALL
</code></pre><h3 id=dependencies>
Dependencies
<a class=anchor href=#dependencies>#</a>
</h3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Bash data-lang=Bash>$ sudo apt install python3<span style=color:#ae81ff>\-</span>pip
$ sudo pip3 install <span style=color:#ae81ff>\-\-</span>upgrade pip
$ pip <span style=color:#ae81ff>\-\-</span>version
pip 21.3.1 from /home/brayden/.local/lib/python3.9/site<span style=color:#ae81ff>\-</span>packages/pip <span style=color:#ae81ff>\(</span>python 3.9<span style=color:#ae81ff>\)</span>
</code></pre></div><h3 id=download-kubespray>
Download kubespray
<a class=anchor href=#download-kubespray>#</a>
</h3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ git clone https://github.com/kubernetes<span style=color:#ae81ff>\-</span>sigs/kubespray.git
$ cd kubespray
$ sudo pip install <span style=color:#ae81ff>\-</span>r requirements.txt
</code></pre></div><h3 id=configure-the-cluster-setup>
Configure the cluster setup
<a class=anchor href=#configure-the-cluster-setup>#</a>
</h3>
<p>Copy the required configuration files, scripts to a dedicated folder to this cluster, in this example, the new folder is named as k8s-100-cluster, as the controller api server will be available at 192.168.8.100.</p>
<pre><code>$ cp \-rfp inventory/sampe inventory/k8s\-100\-cluster
</code></pre>
<p>The major configuration about the cluster is done by the inventory.ini, which consists of four major sections</p>
<ul>
<li><strong>all</strong></li>
<li><strong>kube_control_plane</strong></li>
<li><strong>etcd</strong></li>
<li><strong>kube_node</strong></li>
</ul>
<pre tabindex=0><code>$ vim inventory/k8s\-100\-cluster/inventory.ini

\# \#\# Configure 'ip' variable to bind kubernetes services on a   
\# \#\# different ip than the default iface 
\# \#\# We should set etcd\_member\_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value. 
\[all\] 
**node\-101 ansible\_host=192.168.8.101Â Â  ip=192.168.8.101 etcd\_member\_name=etcd1  
node\-102 ansible\_host=192.168.8.102Â Â  ip=192.168.8.102 etcd\_member\_name=etcd2   
node\-103 ansible\_host=192.168.8.103Â Â  ip=192.168.8.103 etcd\_member\_name=etcd3**

\# \#\# configure a bastion host if your nodes are not directly reachable 
\# \[bastion\] 
\# bastion ansible\_host=x.x.x.x ansible\_user=some\_user 

\[kube\_control\_plane\]
**node\-101 
node\-102 
node\-103**
\[etcd\]
**node\-101 
node\-102 
node\-103**

\[kube\_node\] 
**node\-103**
</code></pre><p>Another important configuration file we shall touch is <code>inventory\k8s-100-cluster\group_vars\all\all.yml</code>.</p>
<p>Here we disable the internal nginx based proxy, and use external load balanced implemented with haproxy.</p>
<pre tabindex=0><code>**apiserver_loadbalancer_domain_name: &quot;lb.npg.intel&quot;**  
**loadbalancer\_apiserver:**  
**addres: 192.168.8.100**  
**port: 8443**  
**loadbalancer\_apiserver\_localhost: false**  
**\#loadbalancer\_apiserver\_port: 6443**  
**\#loadbalancer\_apiserver\_healthcheck\_port: 8081**  
**upstream\_dns\_servers:**  
**\- 8.8.8.8**  
**\- 8.8.4.4**  
\# Set the proxy, will be populated to apt source and container runtime proxy conf
**http\_proxy: &quot;http://child\-prc.intel.com:913&quot;**
**https\_proxy: &quot;http://child\-prc.intel.com:913&quot;**
</code></pre><p>inventory/k8s-100-cluster/group_vars/k8s_cluster/k8s-cluster.yml</p>
<pre tabindex=0><code>**kube\_service\_addresses: 192.168.0.0/18**  
**kube\_pods\_subnet: 192.168.64.0/18**  
**container\_manager: docker \#containerd**  
**kubelet\_deployment\_type: host**
</code></pre><h3 id=required-workaround>
Required Workaround
<a class=anchor href=#required-workaround>#</a>
</h3>
<blockquote>
<p>Only required if docker is selected as the container runtime.</p>
</blockquote>
<p>At the time this memo is being written, the ubuntu 22.04 is not yet GA, so some of the URLs or versions for the docker packages are not valid, hardcode required to fix the issue.</p>
<pre tabindex=0><code>diff \-\-git a/roles/container\-engine/docker/vars/ubuntu.yml b/roles/container\-engine/docker/vars/ubuntu.yml
index 253dbf17..c0077ebf 100644

\-\-\- a/roles/container\-engine/docker/vars/ubuntu.yml
\+\+\+ b/roles/container\-engine/docker/vars/ubuntu.yml
@@ \-17,16 \+17,16 @@ docker\_versioned\_pkg:
  'latest': docker\-ce
  '18.09': docker\-ce=5:18.09.9~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}
  '19.03': docker\-ce=5:19.03.15~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}
- '20.10': docker\-ce=5:20.10.11~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}
- 'stable': docker\-ce=5:20.10.11~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}
+ '20.10': docker\-ce=5:20.10.12~3\-0~ubuntu\-focal
+ 'stable': docker\-ce=5:20.10.12~3\-0~ubuntu\-focal
  'edge': docker\-ce=5:20.10.11~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}

docker\_cli\_versioned\_pkg:

  'latest': docker\-ce\-cli
  '18.09': docker\-ce\-cli=5:18.09.9~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}
  '19.03': docker\-ce\-cli=5:19.03.15~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}

- '20.10': docker\-ce\-cli=5:20.10.11~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}
- 'stable': docker\-ce\-cli=5:20.10.11~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}
+ '20.10': docker\-ce\-cli=5:20.10.12~3\-0~ubuntu\-focal
+ 'stable': docker\-ce\-cli=5:20.10.12~3\-0~ubuntu\-focal
  'edge': docker\-ce\-cli=5:20.10.11~3\-0~ubuntu\-{{ ansible\_distribution\_release|lower }}

docker\_package\_info:

@@ \-44,5 \+44,8 @@ docker\_repo\_info:

repos:
  - &gt;
    deb \[arch={{ host\_architecture }}\] {{ docker\_ubuntu\_repo\_base\_url }}
-     {{ ansible\_distribution\_release|lower }}
+     focal
      stable

diff \-\-git a/roles/kubernetes/node/tasks/main.yml b/roles/kubernetes/node/tasks/main.yml
index a342d940..9118a3e6 100644
--- a/roles/kubernetes/node/tasks/main.yml
+++ b/roles/kubernetes/node/tasks/main.yml
@@ \-107,7 \+107,7 @@
- name: Modprobe nf_conntrack_ipv4
  modprobe:
-   name: nf_conntrack_ipv4
+   name: nf_conntrack

  state: present
  register: modprobe_nf_conntrack_ipv4
  ignore_errors: true \# noqa ignore\-errors
</code></pre><h4 id=update-the-hosts-file>
Update the hosts file
<a class=anchor href=#update-the-hosts-file>#</a>
</h4>
<p>you may want to add one line for the virtual ip:</p>
<pre tabindex=0><code>$ sudo vim /etc/hosts
192.168.8.100 lb.npg.intel
</code></pre><h2 id=provision-the-cluster>
Provision the Cluster
<a class=anchor href=#provision-the-cluster>#</a>
</h2>
<pre><code>$ ansible\-playbook \-i inventory/k8s\-100\-cluster/inventory.ini \-\-become \-\-user=brayden \-\-become\-user=root cluster.yml
</code></pre>
<p>If error occured during the provistion, it&rsquo;s better to reset the cluster, fix the issue, and relaunch the provision.</p>
<pre><code>$ ansible\-playbook \-i inventory/k8s\-100\-cluster/inventory.ini \-\-become \-\-user=brayden \-\-become\-user=root reset.yml
$ ansible\-playbook \-i inventory/k8s\-100\-cluster/inventory.ini \-\-become \-\-user=brayden \-\-become\-user=root cluster.yml
</code></pre>
<h3 id=kubectl-conf-settings>
Kubectl conf settings
<a class=anchor href=#kubectl-conf-settings>#</a>
</h3>
<pre tabindex=0><code>$ cd ~
$ mkdir .kube
$ cp /etc/kubernetes/admin.conf .kube/
$ echo &quot;export KUBECONFIG=/home/brayden/.kube/admin.conf&quot;

$ kubectl get pods \-n kube\-system

NAME                                         READY    STATUS    RESTARTS    AGE
calico\-kube\-controllers\-5788f6558\-kkbf4    1/1    Running    0          38h
calico\-node\-hcdh4                            1/1    Running    0          38h
calico\-node\-xfbcq                            1/1    Running    0          38h
calico\-node\-z7qpt                            1/1    Running    0          38h
coredns\-8474476ff8\-7l69z                     1/1    Running    0          38h
coredns\-8474476ff8\-88                        1/1    Running    0          38h
dns\-autoscaler\-5ffdc7f89d\-jqq5m             1/1    Running    0          38h
kube\-apiserver\-node\-101                     1/1    Running    1 (38h ago) 39h
kube\-apiserver\-node\-102                     1/1    Running    1 (38h ago) 39h
kube\-apiserver\-node\-103                     1/1    Running    1 (38h ago) 39h
kube\-controller\-manager\-node\-101           1/1    Running    1          39h
kube\-controller\-manager\-node\-102           1/1    Running    2 (38h ago) 39h
kube\-controller\-manager\-node\-103           1/1    Running    1          39h
kube\-proxy\-fzlvr                             1/1    Running    0          39h
kube\-proxy\-k9z6g                             1/1    Running    0          39h
kube\-proxy\-wrhd7                             1/1    Running    0          39h
kube\-scheduler\-node\-101                     1/1    Running    2 (38h ago) 39h
kube\-scheduler\-node\-102                     1/1    Running    1          39h
kube\-scheduler\-node\-103                     1/1    Running    1          39h
nodelocaldns\-285cc                            1/1    Running    0          38h
nodelocaldns\-2d477                            1/1    Running    0          38h
nodelocaldns\-cpckx                            1/1    Running    0          38h
</code></pre><h2 id=troubleshootings>
Troubleshootings
<a class=anchor href=#troubleshootings>#</a>
</h2>
<h3 id=failed-to-download-container-images>
Failed to download container images
<a class=anchor href=#failed-to-download-container-images>#</a>
</h3>
<p>Initially, the container runtime is default to containerd, and kubespray uses nerdctl which is docker compatible tools to pull the container images.</p>
<p>kubespray populated the proxy setting for apt source and containerd service according to the all.yml. However, nerdctl is not able to pull the container image, and no luck with --extra-vars to the ansiable-playbook.</p>
<pre><code>ansible\-playbook \-i inventory/k8s\-100\-cluster/inventory.ini \-\-become \-\-user=brayden cluster.yml \-\-extra\-vars &quot;https\_proxy=http://child\-prc.intel.com:913,http\_proxy=http://child\-prc.intel.com:913&quot;
</code></pre>
<p>While manually run the nerdctl from command line could pull the images successfully.</p>
<p>So this memo switch the container runtime to docker, which could pull the images with the proxy settings.</p>
<h3 id=node-102--node-103-failed-to-join-controller-node-101>
Node-102 & Node-103 failed to join controller node-101
<a class=anchor href=#node-102--node-103-failed-to-join-controller-node-101>#</a>
</h3>
<p>One of the error message reads:</p>
<pre><code>error execution phase preflight: couldn't validate the identity of the API Server: configmaps &quot;cluster\-info&quot; is forbidden: User &quot;system:anonymous&quot; cannot get resource &quot;configmaps&quot; in API group &quot;&quot; in the namespace &quot;kube\-public&quot;
</code></pre>
<p>No dedicated effort spent to root cause this issue, a few changes made and the node could join the cluster successfully.</p>
<ul>
<li>Do reset before relaunch the provision procedure. Refer to section &ldquo;Provision the Cluster&rdquo;.</li>
<li>Clean up the ip routing, name server setting.
<ul>
<li>Configure the Ethernet and nameserver explicitely in /etc/netplan/00-installer-config.yaml</li>
</ul>
</li>
</ul>
<pre tabindex=0><code># This is the network config written by 'subiquity'
network:
  ethernets:
    eno1:
      dhcp4: true
      dhcp6: false
      match:
        macaddress: 00:1e:67:e6:14:ad
      nameservers:
        addresses:
        - 10.248.2.5
        - 10.239.27.228
    eno2:
      dhcp4: false
      addresses:
      - 192.168.8.101/24
      match:
        macaddress: 00:1e:67:e6:14:ae

    * Cleanup the /etc/resolve.conf, remove the local addresses.
    * Make sure &quot;sudo apt update&quot; could be executed successfully.
</code></pre><h3 id=coredns-service-crashedbackoff>
coredns service crashedbackoff
<a class=anchor href=#coredns-service-crashedbackoff>#</a>
</h3>
<p>Server&rsquo;s console will continuous show:</p>
<pre><code>[19713.675335] IPVS: rr: UDP 192.168.0.3:53 - no destination available
</code></pre>
<p>coredns is in crashloopBackoff status:</p>
<pre><code>kube-system    coredns\-576cbf47c7\-8phwt    0/1    CrashLoopBackOff     8  
</code></pre>
<p>And the coredns container&rsquo;s log reads</p>
<pre><code>plugin/loop: **Loop** \(127.0.0.1:55953 \-\&gt; :53\) **detected for zone &quot;.&quot;**, see https://coredns.io/plugins/loop\#troubleshooting. Query: &quot;HINFO 4547991504243258144.3688648895315093531.&quot;
</code></pre>
<p>There are also some logs reads</p>
<pre><code>&quot;... 192.168.0.1 connection refused ...&quot;
</code></pre>
<p>This was due to miss-configuration of nameserver, after correct the settings in all.yml, and clean up the /etc/resolv.conf, the issue is fixed.</p>
<h2 id=reference>
Reference
<a class=anchor href=#reference>#</a>
</h2>
<p><a href=https://computingforgeeks.com/deploy-kubernetes-cluster-debian-with-kubespray/>Install Kubernetes Cluster on Debian 10 with Kubespray | ComputingForGeeks</a><br>
<a href="https://www.youtube.com/watch?v=CJ5G4GpqDy0">Deploying kubernetes using Kubespray - YouTube</a><br>
<a href=https://coredns.io/plugins/loop/#troubleshooting>loop (coredns.io)</a><br>
<a href=https://dev.to/mrturkmen/setup-highly-available-kubernetes-cluster-with-haproxy-2dm8>Setup Highly Available Kubernetes Cluster with HAProxy ðŸ‡¬ðŸ‡§ - DEV Community</a><br>
<a href=https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository>Install Docker Engine on Ubuntu | Docker Documentation</a><br>
<a href=https://stdworkflow.com/1247/k8s-join-user-system-anonymous-cannot-get-resource-configmaps-in-api-group-in-the-namespace>k8s join User â€œsystem:anonymousâ€œ cannot get resource â€œconfigmapsâ€œ in API group â€œâ€œ in the namespace - stdworkflow</a></p>
</article>
<footer class=book-footer>
<div class="flex flex-wrap justify-between">
</div>
<script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>
</footer>
<div class=book-comments>
</div>
<label for=menu-control class="hidden book-menu-overlay"></label>
</div>
<aside class=book-toc>
<div class=book-toc-content>
<nav id=TableOfContents>
<ul>
<li><a href=#introduction>Introduction</a></li>
<li><a href=#prerequisites>Prerequisites</a></li>
<li><a href=#detailed-steps>Detailed Steps</a>
<ul>
<li><a href=#disable-swap>Disable swap</a></li>
<li><a href=#enable-passwordless-login-via-ssh>Enable passwordless login via ssh</a></li>
<li><a href=#enable-passwordless-sudo>Enable passwordless sudo</a></li>
<li><a href=#dependencies>Dependencies</a></li>
<li><a href=#download-kubespray>Download kubespray</a></li>
<li><a href=#configure-the-cluster-setup>Configure the cluster setup</a></li>
<li><a href=#required-workaround>Required Workaround</a></li>
</ul>
</li>
<li><a href=#provision-the-cluster>Provision the Cluster</a>
<ul>
<li><a href=#kubectl-conf-settings>Kubectl conf settings</a></li>
</ul>
</li>
<li><a href=#troubleshootings>Troubleshootings</a>
<ul>
<li><a href=#failed-to-download-container-images>Failed to download container images</a></li>
<li><a href=#node-102--node-103-failed-to-join-controller-node-101>Node-102 & Node-103 failed to join controller node-101</a></li>
<li><a href=#coredns-service-crashedbackoff>coredns service crashedbackoff</a></li>
</ul>
</li>
<li><a href=#reference>Reference</a></li>
</ul>
</nav>
</div>
</aside>
</main>
</body>
</html>